# machine-learning-1-week-5-model-selection-solved
**TO GET THIS SOLUTION VISIT:** [Machine Learning 1 Week 5-Model Selection Solved](https://www.ankitcodinghub.com/product/machine-learning-1-week-5-model-selection-solved/)


---

ğŸ“© **If you need this solution or have special requests:** **Email:** ankitcoding@gmail.com  
ğŸ“± **WhatsApp:** +1 419 877 7882  
ğŸ“„ **Get a quote instantly using this form:** [Ask Homework Questions](https://www.ankitcodinghub.com/services/ask-homework-questions/)

*We deliver fast, professional, and affordable academic help.*

---

<h2>Description</h2>



<div class="kk-star-ratings kksr-auto kksr-align-center kksr-valign-top" data-payload="{&quot;align&quot;:&quot;center&quot;,&quot;id&quot;:&quot;98754&quot;,&quot;slug&quot;:&quot;default&quot;,&quot;valign&quot;:&quot;top&quot;,&quot;ignore&quot;:&quot;&quot;,&quot;reference&quot;:&quot;auto&quot;,&quot;class&quot;:&quot;&quot;,&quot;count&quot;:&quot;0&quot;,&quot;legendonly&quot;:&quot;&quot;,&quot;readonly&quot;:&quot;&quot;,&quot;score&quot;:&quot;0&quot;,&quot;starsonly&quot;:&quot;&quot;,&quot;best&quot;:&quot;5&quot;,&quot;gap&quot;:&quot;4&quot;,&quot;greet&quot;:&quot;Rate this product&quot;,&quot;legend&quot;:&quot;0\/5 - (0 votes)&quot;,&quot;size&quot;:&quot;24&quot;,&quot;title&quot;:&quot;Machine Learning 1 Week 5-Model Selection Solved&quot;,&quot;width&quot;:&quot;0&quot;,&quot;_legend&quot;:&quot;{score}\/{best} - ({count} {votes})&quot;,&quot;font_factor&quot;:&quot;1.25&quot;}">

<div class="kksr-stars">

<div class="kksr-stars-inactive">
            <div class="kksr-star" data-star="1" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" data-star="2" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" data-star="3" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" data-star="4" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" data-star="5" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
    </div>

<div class="kksr-stars-active" style="width: 0px;">
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
    </div>
</div>


<div class="kksr-legend" style="font-size: 19.2px;">
            <span class="kksr-muted">Rate this product</span>
    </div>
    </div>
<div class="page" title="Page 1">
<div class="layoutArea">
<div class="column">
Exercise 1: Bias and Variance of Mean Estimators

Assume we have an estimator Î¸Ë† for a parameter Î¸. The bias of the estimator Î¸Ë† is the difference between the true value for the estimator, and its expected value

Bias(Î¸Ë†) = Eô°Î¸Ë† âˆ’ Î¸ô°.

If Bias(Î¸Ë†) = 0, then Î¸Ë† is called unbiased. The variance of the estimator Î¸Ë† is the expected square deviation

</div>
</div>
<div class="layoutArea">
<div class="column">
from its expected value

Var(Î¸Ë†) = Eô°(Î¸Ë† âˆ’ E[Î¸Ë†])2ô°. The mean squared error of the estimator Î¸Ë† is

Error(Î¸Ë†) = Eô°(Î¸Ë† âˆ’ Î¸)2ô° = Bias(Î¸Ë†)2 + Var(Î¸Ë†).

Let X1, . . . , XN be a sample of i.i.d random variables. Assume that Xi has

Calculate the bias, variance and mean squared error of the mean estimator:

1 ô°‰N

</div>
<div class="column">
mean Î¼ and

</div>
<div class="column">
variance Ïƒ2.

</div>
</div>
<div class="layoutArea">
<div class="column">
Î¼Ë† = Î± Â· N

Exercise 2: Bias-Variance Decomposition for Classification (30 P)

</div>
</div>
<div class="layoutArea">
<div class="column">
where Î± is a parameter between 0 and 1.

</div>
</div>
<div class="layoutArea">
<div class="column">
The bias-variance decomposition usually applies to regression data. In this exercise, we would like to obtain similar decomposition for classification, in particular, when the prediction is given as a probability distribution over C classes. Let P = [P1, . . . , PC ] be the ground truth class distribution associated to a particular input pattern. Assume a random estimator of class probabilities PË† = [PË†1,â€¦,PË†C] for the same input pattern. The error function is given by the expected KL-divergence between the ground truth and the estimated probability distribution:

Error = Eô°DKL(P ||PË†)ô° = Eô° ô°ŒCi=1 Pi log(Pi/PË†i)ô°.

First, we would like to determine the mean of of the class distribution estimator PË†. We define the mean as the distribution that minimizes its expected KL divergence from the the class distribution estimator, that is, the distribution R that optimizes

min Eô°DKL(R||PË†)ô°. R

</div>
</div>
<div class="layoutArea">
<div class="column">
(a) Show that the solution to the optimization problem above is given by expEô°logPË†iô°

</div>
</div>
<div class="layoutArea">
<div class="column">
R=[R1,â€¦,RC] where Ri=ô°ŒjexpEô°logPË†jô° âˆ€1â‰¤iâ‰¤C.

(Hint: To implement the positivity constraint on R, you can reparameterize its components as Ri = exp(Zi),

</div>
</div>
<div class="layoutArea">
<div class="column">
i=1

</div>
</div>
<div class="layoutArea">
<div class="column">
X i

</div>
</div>
<div class="layoutArea">
<div class="column">
and minimize the objective w.r.t. Z.) (b) Prove the bias-variance decomposition

Error(PË†) = Bias(PË†) + Var(PË†) where the error, bias and variance are given by

Error(PË†) = Eô°DKL(P ||PË†)ô°, Bias(PË†) = DKL(P ||R), Var(PË†) = Eô°DKL(R||PË†)ô°. (Hint: as a first step, it can be useful to show that E[log Ri âˆ’ log PË†i] does not depend on the index i.)

Exercise 3: Programming (50 P)

Download the programming files on ISIS and follow the instructions.

</div>
</div>
</div>
<div class="page" title="Page 2">
<div class="section">
<div class="layoutArea">
<div class="column">
Exercise sheet 5 (programming) [WiSe 2021/22] Machine Learning 1

</div>
</div>
<div class="layoutArea">
<div class="column">
Part 1: The James-Stein Estimator (20 P)

Let x1, â€¦, xN âˆˆ Rd be independent draws from a multivariate Gaussian distribution with mean vector Î¼ and covariance matrix Î£ = Ïƒ2I. It can be shown that the maximum-likelihood estimator of the mean parameter Î¼ is the empirical mean given by:

1N

Î¼Ë† M L = N âˆ‘ x i

i=1

Maximum-likelihood appears to be a strong estimator. However, it was demonstrated that the following estimator

(d âˆ’ 2) â‹… Ïƒ2

Î¼Ë† J S = ( 1 âˆ’ N ) Î¼Ë† M L

(a shrinked version of the maximum-likelihood estimator towards the origin) has actually a smaller distance from the true mean when d â‰¥ 3. This however assumes knowledge of the variance of the distribution for which the mean is estimated. This estimator is called the James-Stein estimator. While the proof is a bit involved, this fact can be easily demonstrated empirically through simulation. This is the object of this exercise.

The code below draws ten 50-dimensional points from a normal distribution with mean vector Î¼ = (1, â€¦, 1) and covariance Î£ = I. In [1]:

import numpy

def getdata(seed):

n = 10 # data points

d = 50 # dimensionality of data m = numpy.ones([d]) # true mean

s = 1.0 # true standard deviation

<pre>    rstate = numpy.random.mtrand.RandomState(seed)
    X = rstate.normal(0,1,[n,d])*s+m
</pre>
return X,m,s

The following function computes the maximum likelihood estimator from a sample of the data assumed to be generated by a Gaussian distribution:

In [2]: def ML(X):

return X.mean(axis=0)

Implementing the James-Stein Estimator (10 P)

Based on the ML estimator function, write a function that receives as input the data (Xi)ni = 1 and the (known) variance Ïƒ2 of the generating distribution, and computes the James-Stein estimator

In [3]:

def JS(X,s):

# REPLACE BY YOUR CODE import solutions

m_JS = solutions.JS(X,s) ###

return m_JS

Comparing the ML and James-Stein Estimators (10 P)

We would like to compute the error of the maximum likelihood estimator and the James-Stein estimator for 100 different samples (where each sample consistsof10drawsgeneratedbythefunction getdata withadifferentrandomseed).Here,forreproducibility,weuseseedsfrom0to99.Theerror should be measured as the Euclidean distance between the true mean vector and the estimated mean vector.

Compute the maximum-likelihood and James-Stein estimations. Measure the error of these estimations.

Build a scatter plot comparing these errors for different samples.

</div>
</div>
<div class="layoutArea">
<div class="column">
â€– Î¼Ë† M L â€– 2

</div>
</div>
</div>
</div>
<div class="page" title="Page 3">
<div class="section">
<div class="layoutArea">
<div class="column">
In [4]:

%matplotlib inline

### REPLACE BY YOUR CODE import solutions solutions.compare_ML_JS() ###

Part 2: Bias/Variance Decomposition (30 P)

In this part, we would like to implement a procedure to find the bias and variance of different predictors. We consider one for regression and one for classification. These predictors are available in the module utils.

utils.ParzenRegressor : A regression method based on Parzen window. The hyperparameter corresponds to the scale of the Parzen window. A large scale creates a more rigid model. A small scale creates a more flexible one.

utils.ParzenClassifier : A classification method based on Parzen window. The hyperparameter corresponds to the scale of the Parzen window. A large scale creates a more rigid model. A small scale creates a more flexible one. Note that instead of returning a single class for a given data point, it outputs a probability distribution over the set of possible classes.

Each class of predictor implements the following three methods:

__init__(self,parameter): Createaninstanceofthepredictorwithacertainscaleparameter. fit(self,X,T): Fitthepredictortothedata(asetofdatapoints X andtargets T). predict(self,X): Compute the output values arbitrary inputs X .

To compute the bias and variance estimates, we require multiple samples from the training set for a single set of observation data. To acomplish this, we utilizethe Sampler classprovided.Thesamplerisinitializedwiththetrainingdataandpassedtothemethodforestimatingbiasandvariance,whereits function sampler.sample() iscalledrepeatedlyinordertofitmultiplemodelsandcreateanensembleofpredictionforeachtestdatapoint.

Regression Case (15 P)

For the regression case, Bias, Variance and Error are given by:

Bias(Y)2 = (EY[Y âˆ’ T])2 Var(Y) = EY[(Y âˆ’ EY[Y])2] Error(Y) = EY[(Y âˆ’ T)2]

Task: Implement the KL-based Bias-Variance Decomposition defined above. The function should repeatedly sample training sets from the sampler (as many times as specified by the argument nbsamples), learn the predictor on them, and evaluate the variance on the out-of-sample distribution given by X and T.

In [5]:

def biasVarianceRegression(sampler, predictor, X, T, nbsamples):

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€“ # TODO: REPLACE BY YOUR CODE

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€“ import solutions

<pre>    bias,variance = solutions.biasVarianceRegression(sampler, predictor, X, T, nbsamples=nbsamples)
</pre>
<pre>    # --------------------------------
</pre>
return bias,variance

Your implementation can be tested with the following code:

</div>
</div>
</div>
</div>
<div class="page" title="Page 4">
<div class="section">
<div class="layoutArea">
<div class="column">
In [6]:

import utils,numpy

%matplotlib inline utils.plotBVE(utils.Housing,numpy.logspace(-6,3,num=30),utils.ParzenRegressor,biasVarianceRegression,â€™Housing Reg ressionâ€™)

Classification Case (15 P)

We consider here the Kullback-Leibler divergence as a measure of classification error, as derived in the exercise, the Bias, Variance decomposition for such error is:

Bias(Y) = DKL(T | | R) Var(Y) = EY[DKL(R | | Y)] Error(Y) = EY[DKL(T | | Y)]

where R is the distribution that minimizes its expected KL divergence from the estimator of probability distribution Y (see the theoretical exercise for how it is computed exactly), and where T is the target class distribution.

Task: Implement the KL-based Bias-Variance Decomposition defined above. The function should repeatedly sample training sets from the sampler (as many times as specified by the argument nbsamples), learn the predictor on them, and evaluate the variance on the out-of-sample distribution given by X and T.

In [7]:

def biasVarianceClassification(sampler, predictor, X, T, nbsamples=25):

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€“ # TODO: REPLACE BY YOUR CODE

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€“ import solutions

<pre>    bias,variance = solutions.biasVarianceClassification(sampler, predictor, X, T, nbsamples=nbsamples)
</pre>
<pre>    # --------------------------------
</pre>
return bias,variance

Your implementation can be tested with the following code:

</div>
</div>
</div>
</div>
<div class="page" title="Page 5">
<div class="section">
<div class="layoutArea">
<div class="column">
In [8]:

import utils,numpy

%matplotlib inline utils.plotBVE(utils.Yeast,numpy.logspace(-6,3,num=30),utils.ParzenClassifier,biasVarianceClassification,â€™Yeast Cl assificationâ€™)

</div>
</div>
</div>
</div>
<div class="page" title="Page 6"></div>
<div class="page" title="Page 7"></div>
<div class="page" title="Page 8"></div>
